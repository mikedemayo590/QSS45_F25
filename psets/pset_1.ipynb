{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time # to see if this is fast or not\n",
    "\n",
    "# Global hyperparameters\n",
    "learning_rate = 0.01\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implement multivariate linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_linear_regression(X, y):\n",
    "    n, k = X.shape\n",
    "    \n",
    "    # adding the intercept col so when calculating can start with b0 (otherwise would just give slopes in beta)\n",
    "    X_b = np.hstack((np.ones((n, 1)), X))\n",
    "    \n",
    "    beta = np.random.randint(1, 100, size=(k+1,))\n",
    "    loss_hist = []\n",
    "    for i in range(iterations):\n",
    "        # making a prediction of y\n",
    "        y_pred = np.sum(beta * X_b, axis=1)\n",
    "        \n",
    "        #figure out loss and updating it into the array\n",
    "        loss = (1/n) * np.sum((y_pred - y)**2)\n",
    "        loss_hist.append(loss)\n",
    "        # calculating every gradient for each beta and reshaping to do matrix mult properly\n",
    "        gradients = (1/n) * np.sum((y_pred - y).reshape(-1,1) * X_b, axis=0)\n",
    "        \n",
    "        # Update array of all betas at once\n",
    "        beta = beta - learning_rate * gradients\n",
    "\n",
    "    \n",
    "    # returning finished array of final betas after iterations\n",
    "    return beta, loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implement multivariate logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mult_logistic_regression(X, y):\n",
    "    # initial defs and figuring out shape of array X\n",
    "    n, k = X.shape\n",
    "    X_b = np.hstack((np.ones((n, 1)), X))\n",
    "    beta = np.random.randn(k+1)\n",
    "    loss_hist = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "    \n",
    "        z = np.sum(beta * X_b, axis=1)\n",
    "        y_pred = 1 / (1 + np.exp(-z))\n",
    "\n",
    "        loss = -(1/n) * np.sum(y*np.log(y_pred + 1e-8) + (1-y)*np.log(1-y_pred + 1e-8))\n",
    "        loss_hist.append(loss)\n",
    "\n",
    "       \n",
    "        gradients = (1/n) * np.sum((y_pred - y).reshape(-1,1) * X_b, axis=0)\n",
    "\n",
    "        beta = beta - learning_rate * gradients\n",
    "\n",
    "\n",
    "    return beta, loss_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing both with random generated numbers as data and seeing speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Weights (first 5): [5.01479427 2.00192134 1.99234443 2.00173652 2.00647124]\n",
      "Final Linear Loss: 0.9991851346742019\n",
      "Time (Linear): 0.811 seconds\n",
      "\n",
      "Logistic Regression Weights (first 5): [-0.71425694  0.48747659  0.42840825  0.46079284  0.67482435]\n",
      "Final Logistic Loss: 0.29474897044158366\n",
      "Time (Logistic): 1.021 seconds\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Linear regression dataset (n=10k, k=10)\n",
    "X_lin = np.random.randn(10000, 10)\n",
    "y_lin = 5 + np.dot(X_lin, np.ones(10)*2) + np.random.randn(10000)\n",
    "\n",
    "# Logistic regression dataset (n=10k, k=10)\n",
    "X_log = np.random.randn(10000, 10)\n",
    "z = -1 + np.dot(X_log, np.ones(10))\n",
    "prob = 1 / (1 + np.exp(-z))\n",
    "y_log = (prob > 0.5).astype(int)\n",
    "\n",
    "# ===== Run and time Linear Regression =====\n",
    "start = time.time()\n",
    "beta_lin, loss_lin = mult_linear_regression(X_lin, y_lin)\n",
    "end = time.time()\n",
    "print(\"Linear Regression Weights (first 5):\", beta_lin[:5])\n",
    "print(\"Final Linear Loss:\", loss_lin[-1])\n",
    "print(\"Time (Linear):\", round(end-start, 4), \"seconds\\n\")\n",
    "\n",
    "# ===== Run and time Logistic Regression =====\n",
    "start = time.time()\n",
    "beta_log, loss_log = mult_logistic_regression(X_log, y_log)\n",
    "end = time.time()\n",
    "print(\"Logistic Regression Weights (first 5):\", beta_log[:5])\n",
    "print(\"Final Logistic Loss:\", loss_log[-1])\n",
    "print(\"Time (Logistic):\", round(end-start, 4), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
