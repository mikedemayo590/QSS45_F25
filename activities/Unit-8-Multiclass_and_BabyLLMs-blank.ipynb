{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "36b081e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from palmerpenguins import load_penguins\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0422a338",
   "metadata": {},
   "outputs": [],
   "source": [
    "peng = load_penguins()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c7748",
   "metadata": {},
   "source": [
    "#### Pre-processing data\n",
    "* Using pd.factorize(), transform the categorical variables into numbers.\n",
    "* Afterward, impute the missing values for each column by the mean.\n",
    "* Then, using train_test_split, generate a 80-20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a3de649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code\n",
    "peng[\"species\"],uniq_spec = pd.factorize( peng[\"species\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d2abb44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gn/hl68wgxj1dg3j7wh7shxt0jw0000gp/T/ipykernel_81938/3359383414.py:3: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  peng[\"bill_length_mm\"].fillna(peng[\"bill_length_mm\"].mean(), inplace = True)\n",
      "/var/folders/gn/hl68wgxj1dg3j7wh7shxt0jw0000gp/T/ipykernel_81938/3359383414.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  peng[\"bill_depth_mm\"].fillna(peng[\"bill_depth_mm\"].mean(), inplace = True)\n",
      "/var/folders/gn/hl68wgxj1dg3j7wh7shxt0jw0000gp/T/ipykernel_81938/3359383414.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  peng[\"flipper_length_mm\"].fillna(peng[\"flipper_length_mm\"].mean(), inplace = True)\n",
      "/var/folders/gn/hl68wgxj1dg3j7wh7shxt0jw0000gp/T/ipykernel_81938/3359383414.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  peng[\"body_mass_g\"].fillna(peng[\"body_mass_g\"].mean(), inplace = True)\n",
      "/var/folders/gn/hl68wgxj1dg3j7wh7shxt0jw0000gp/T/ipykernel_81938/3359383414.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  peng[\"sex\"].fillna(peng[\"sex\"].mode()[0], inplace = True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>island</th>\n",
       "      <th>bill_length_mm</th>\n",
       "      <th>bill_depth_mm</th>\n",
       "      <th>flipper_length_mm</th>\n",
       "      <th>body_mass_g</th>\n",
       "      <th>sex</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.10000</td>\n",
       "      <td>18.70000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>3750.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39.50000</td>\n",
       "      <td>17.40000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>3800.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.30000</td>\n",
       "      <td>18.00000</td>\n",
       "      <td>195.000000</td>\n",
       "      <td>3250.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.92193</td>\n",
       "      <td>17.15117</td>\n",
       "      <td>200.915205</td>\n",
       "      <td>4201.754386</td>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36.70000</td>\n",
       "      <td>19.30000</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>3450.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>55.80000</td>\n",
       "      <td>19.80000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>43.50000</td>\n",
       "      <td>18.10000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>3400.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>49.60000</td>\n",
       "      <td>18.20000</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>3775.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>50.80000</td>\n",
       "      <td>19.00000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>4100.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>50.20000</td>\n",
       "      <td>18.70000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>3775.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     species  island  bill_length_mm  bill_depth_mm  flipper_length_mm  \\\n",
       "0          0       0        39.10000       18.70000         181.000000   \n",
       "1          0       0        39.50000       17.40000         186.000000   \n",
       "2          0       0        40.30000       18.00000         195.000000   \n",
       "3          0       0        43.92193       17.15117         200.915205   \n",
       "4          0       0        36.70000       19.30000         193.000000   \n",
       "..       ...     ...             ...            ...                ...   \n",
       "339        2       2        55.80000       19.80000         207.000000   \n",
       "340        2       2        43.50000       18.10000         202.000000   \n",
       "341        2       2        49.60000       18.20000         193.000000   \n",
       "342        2       2        50.80000       19.00000         210.000000   \n",
       "343        2       2        50.20000       18.70000         198.000000   \n",
       "\n",
       "     body_mass_g  sex  year  \n",
       "0    3750.000000    0  2007  \n",
       "1    3800.000000    1  2007  \n",
       "2    3250.000000    1  2007  \n",
       "3    4201.754386    0  2007  \n",
       "4    3450.000000    1  2007  \n",
       "..           ...  ...   ...  \n",
       "339  4000.000000    0  2009  \n",
       "340  3400.000000    1  2009  \n",
       "341  3775.000000    0  2009  \n",
       "342  4100.000000    0  2009  \n",
       "343  3775.000000    1  2009  \n",
       "\n",
       "[344 rows x 8 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filling missing vals\n",
    "\n",
    "peng[\"bill_length_mm\"].fillna(peng[\"bill_length_mm\"].mean(), inplace = True)\n",
    "peng[\"bill_depth_mm\"].fillna(peng[\"bill_depth_mm\"].mean(), inplace = True)\n",
    "peng[\"flipper_length_mm\"].fillna(peng[\"flipper_length_mm\"].mean(), inplace = True)\n",
    "peng[\"body_mass_g\"].fillna(peng[\"body_mass_g\"].mean(), inplace = True)\n",
    "peng[\"sex\"].fillna(peng[\"sex\"].mode()[0], inplace = True)\n",
    "peng[\"sex\"],uniq_sex = pd.factorize(peng[\"sex\"])\n",
    "peng[\"island\"],uniq_island_vals = pd.factorize(peng[\"island\"])\n",
    "\n",
    "peng\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39e54e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = peng.drop(\"species\", axis=1)   # all columns except target\n",
    "y = peng[\"species\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d619c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,       # 20% test, 80% train\n",
    "    random_state=42,     # ensures reproducibility\n",
    ")\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.to_numpy(), dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.to_numpy(), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b05ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class penguinNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(penguinNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cae74a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters / You can vary this as you wish\n",
    "input_size = 7\n",
    "hidden_size = 16\n",
    "output_size = 3\n",
    "model = penguinNet(input_size, hidden_size, output_size)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "322f4faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 0.6557\n",
      "Epoch [20/1000], Loss: 0.1548\n",
      "Epoch [30/1000], Loss: 0.0328\n",
      "Epoch [40/1000], Loss: 0.0170\n",
      "Epoch [50/1000], Loss: 0.0113\n",
      "Epoch [60/1000], Loss: 0.0084\n",
      "Epoch [70/1000], Loss: 0.0064\n",
      "Epoch [80/1000], Loss: 0.0052\n",
      "Epoch [90/1000], Loss: 0.0044\n",
      "Epoch [100/1000], Loss: 0.0038\n",
      "Epoch [110/1000], Loss: 0.0033\n",
      "Epoch [120/1000], Loss: 0.0029\n",
      "Epoch [130/1000], Loss: 0.0025\n",
      "Epoch [140/1000], Loss: 0.0023\n",
      "Epoch [150/1000], Loss: 0.0020\n",
      "Epoch [160/1000], Loss: 0.0018\n",
      "Epoch [170/1000], Loss: 0.0016\n",
      "Epoch [180/1000], Loss: 0.0015\n",
      "Epoch [190/1000], Loss: 0.0013\n",
      "Epoch [200/1000], Loss: 0.0012\n",
      "Epoch [210/1000], Loss: 0.0011\n",
      "Epoch [220/1000], Loss: 0.0010\n",
      "Epoch [230/1000], Loss: 0.0009\n",
      "Epoch [240/1000], Loss: 0.0009\n",
      "Epoch [250/1000], Loss: 0.0008\n",
      "Epoch [260/1000], Loss: 0.0007\n",
      "Epoch [270/1000], Loss: 0.0007\n",
      "Epoch [280/1000], Loss: 0.0006\n",
      "Epoch [290/1000], Loss: 0.0006\n",
      "Epoch [300/1000], Loss: 0.0005\n",
      "Epoch [310/1000], Loss: 0.0005\n",
      "Epoch [320/1000], Loss: 0.0005\n",
      "Epoch [330/1000], Loss: 0.0005\n",
      "Epoch [340/1000], Loss: 0.0004\n",
      "Epoch [350/1000], Loss: 0.0004\n",
      "Epoch [360/1000], Loss: 0.0004\n",
      "Epoch [370/1000], Loss: 0.0004\n",
      "Epoch [380/1000], Loss: 0.0003\n",
      "Epoch [390/1000], Loss: 0.0003\n",
      "Epoch [400/1000], Loss: 0.0003\n",
      "Epoch [410/1000], Loss: 0.0003\n",
      "Epoch [420/1000], Loss: 0.0003\n",
      "Epoch [430/1000], Loss: 0.0003\n",
      "Epoch [440/1000], Loss: 0.0003\n",
      "Epoch [450/1000], Loss: 0.0002\n",
      "Epoch [460/1000], Loss: 0.0002\n",
      "Epoch [470/1000], Loss: 0.0002\n",
      "Epoch [480/1000], Loss: 0.0002\n",
      "Epoch [490/1000], Loss: 0.0002\n",
      "Epoch [500/1000], Loss: 0.0002\n",
      "Epoch [510/1000], Loss: 0.0002\n",
      "Epoch [520/1000], Loss: 0.0002\n",
      "Epoch [530/1000], Loss: 0.0002\n",
      "Epoch [540/1000], Loss: 0.0002\n",
      "Epoch [550/1000], Loss: 0.0002\n",
      "Epoch [560/1000], Loss: 0.0002\n",
      "Epoch [570/1000], Loss: 0.0001\n",
      "Epoch [580/1000], Loss: 0.0001\n",
      "Epoch [590/1000], Loss: 0.0001\n",
      "Epoch [600/1000], Loss: 0.0001\n",
      "Epoch [610/1000], Loss: 0.0001\n",
      "Epoch [620/1000], Loss: 0.0001\n",
      "Epoch [630/1000], Loss: 0.0001\n",
      "Epoch [640/1000], Loss: 0.0001\n",
      "Epoch [650/1000], Loss: 0.0001\n",
      "Epoch [660/1000], Loss: 0.0001\n",
      "Epoch [670/1000], Loss: 0.0001\n",
      "Epoch [680/1000], Loss: 0.0001\n",
      "Epoch [690/1000], Loss: 0.0001\n",
      "Epoch [700/1000], Loss: 0.0001\n",
      "Epoch [710/1000], Loss: 0.0001\n",
      "Epoch [720/1000], Loss: 0.0001\n",
      "Epoch [730/1000], Loss: 0.0001\n",
      "Epoch [740/1000], Loss: 0.0001\n",
      "Epoch [750/1000], Loss: 0.0001\n",
      "Epoch [760/1000], Loss: 0.0001\n",
      "Epoch [770/1000], Loss: 0.0001\n",
      "Epoch [780/1000], Loss: 0.0001\n",
      "Epoch [790/1000], Loss: 0.0001\n",
      "Epoch [800/1000], Loss: 0.0001\n",
      "Epoch [810/1000], Loss: 0.0001\n",
      "Epoch [820/1000], Loss: 0.0001\n",
      "Epoch [830/1000], Loss: 0.0001\n",
      "Epoch [840/1000], Loss: 0.0001\n",
      "Epoch [850/1000], Loss: 0.0001\n",
      "Epoch [860/1000], Loss: 0.0001\n",
      "Epoch [870/1000], Loss: 0.0001\n",
      "Epoch [880/1000], Loss: 0.0001\n",
      "Epoch [890/1000], Loss: 0.0001\n",
      "Epoch [900/1000], Loss: 0.0001\n",
      "Epoch [910/1000], Loss: 0.0001\n",
      "Epoch [920/1000], Loss: 0.0001\n",
      "Epoch [930/1000], Loss: 0.0001\n",
      "Epoch [940/1000], Loss: 0.0001\n",
      "Epoch [950/1000], Loss: 0.0001\n",
      "Epoch [960/1000], Loss: 0.0001\n",
      "Epoch [970/1000], Loss: 0.0000\n",
      "Epoch [980/1000], Loss: 0.0000\n",
      "Epoch [990/1000], Loss: 0.0000\n",
      "Epoch [1000/1000], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass: compute predictions and loss\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    # Backward pass: compute gradients and update parameters\n",
    "    optimizer.zero_grad()  # Reset gradients to zero\n",
    "    loss.backward()        # Backpropagate the error\n",
    "    optimizer.step()       # Update parameters\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741f0cd9",
   "metadata": {},
   "source": [
    "#### Run this to evaluate your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c9c3218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    # Forward pass on the test set\n",
    "    outputs = model(X_test_tensor)\n",
    "    \n",
    "    # Get predicted classes\n",
    "    _, ypred = torch.max(outputs, 1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    total = y_test_tensor.size(0)\n",
    "    correct = (ypred == y_test_tensor).sum().item()\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5c6392",
   "metadata": {},
   "source": [
    "### Generate a confusion matrix of the classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04e8f4c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_true = y_test_tensor.numpy()\n",
    "y_pred = ypred.numpy()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap='Greens')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64b4de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1695fdb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78f84d12",
   "metadata": {},
   "source": [
    "# Word2Vec: The Prehistoric LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f2bc8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1bbcdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/mikedemayo/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mikedemayo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK data files\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b22fa03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentences:\n",
      "['asian', 'exporters', 'fear', 'damage', 'from', 'u', 's', 'japan', 'rift', 'mounting', 'trade', 'friction', 'between', 'the', 'u', 's', 'and', 'japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'asia', 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', 'reaching', 'economic', 'damage', 'businessmen', 'and', 'officials', 'said', 'they', 'told', 'reuter', 'correspondents', 'in', 'asian', 'capitals', 'a', 'u', 's', 'move', 'against', 'japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'u', 's', 'and', 'lead', 'to', 'curbs', 'on', 'american', 'imports', 'of', 'their', 'products', 'but', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'long', 'run', 'in', 'the', 'short', 'term', 'tokyo', 's', 'loss', 'might', 'be', 'their', 'gain', 'the', 'u', 's', 'has', 'said', 'it', 'will', 'impose', 'mln', 'dlrs', 'of', 'tariffs', 'on', 'imports', 'of', 'japanese', 'electronics', 'goods', 'on', 'april', 'in', 'retaliation', 'for', 'japan', 's', 'alleged', 'failure', 'to', 'stick', 'to', 'a', 'pact', 'not', 'to', 'sell', 'semiconductors', 'on', 'world', 'markets', 'at', 'below', 'cost', 'unofficial', 'japanese', 'estimates', 'put', 'the', 'impact', 'of', 'the', 'tariffs', 'at', 'billion', 'dlrs', 'and', 'spokesmen', 'for', 'major', 'electronics', 'firms', 'said', 'they', 'would', 'virtually', 'halt', 'exports', 'of', 'products', 'hit', 'by', 'the', 'new', 'taxes', 'we', 'wouldn', 't', 'be', 'able', 'to', 'do', 'business', 'said', 'a', 'spokesman', 'for', 'leading', 'japanese', 'electronics', 'firm', 'matsushita', 'electric', 'industrial', 'co', 'ltd', 'lt', 'mc', 't', 'if', 'the', 'tariffs', 'remain', 'in', 'place', 'for', 'any', 'length', 'of', 'time', 'beyond', 'a', 'few', 'months', 'it', 'will', 'mean', 'the', 'complete', 'erosion', 'of', 'exports', 'of', 'goods', 'subject', 'to', 'tariffs', 'to', 'the', 'u', 's', 'said', 'tom', 'murtha', 'a', 'stock', 'analyst', 'at', 'the', 'tokyo', 'office', 'of', 'broker', 'lt', 'james', 'capel', 'and', 'co', 'in', 'taiwan', 'businessmen', 'and', 'officials', 'are', 'also', 'worried', 'we', 'are', 'aware', 'of', 'the', 'seriousness', 'of', 'the', 'u', 's', 'threat', 'against', 'japan', 'because', 'it', 'serves', 'as', 'a', 'warning', 'to', 'us', 'said', 'a', 'senior', 'taiwanese', 'trade', 'official', 'who', 'asked', 'not', 'to', 'be', 'named', 'taiwan', 'had', 'a', 'trade', 'trade', 'surplus', 'of', 'billion', 'dlrs', 'last', 'year', 'pct', 'of', 'it', 'with', 'the', 'u', 's', 'the', 'surplus', 'helped', 'swell', 'taiwan', 's', 'foreign', 'exchange', 'reserves', 'to', 'billion', 'dlrs', 'among', 'the', 'world', 's', 'largest', 'we', 'must', 'quickly', 'open', 'our', 'markets', 'remove', 'trade', 'barriers', 'and', 'cut', 'import', 'tariffs', 'to', 'allow', 'imports', 'of', 'u', 's', 'products', 'if', 'we', 'want', 'to', 'defuse', 'problems', 'from', 'possible', 'u', 's', 'retaliation', 'said', 'paul', 'sheen', 'chairman', 'of', 'textile', 'exporters', 'lt', 'taiwan', 'safe', 'group', 'a', 'senior', 'official', 'of', 'south', 'korea', 's', 'trade', 'promotion', 'association', 'said', 'the', 'trade', 'dispute', 'between', 'the', 'u', 's', 'and', 'japan', 'might', 'also', 'lead', 'to', 'pressure', 'on', 'south', 'korea', 'whose', 'chief', 'exports', 'are', 'similar', 'to', 'those', 'of', 'japan', 'last', 'year', 'south', 'korea', 'had', 'a', 'trade', 'surplus', 'of', 'billion', 'dlrs', 'with', 'the', 'u', 's', 'up', 'from', 'billion', 'dlrs', 'in', 'in', 'malaysia', 'trade', 'officers', 'and', 'businessmen', 'said', 'tough', 'curbs', 'against', 'japan', 'might', 'allow', 'hard', 'hit', 'producers', 'of', 'semiconductors', 'in', 'third', 'countries', 'to', 'expand', 'their', 'sales', 'to', 'the', 'u', 's', 'in', 'hong', 'kong', 'where', 'newspapers', 'have', 'alleged', 'japan', 'has', 'been', 'selling', 'below', 'cost', 'semiconductors', 'some', 'electronics', 'manufacturers', 'share', 'that', 'view', 'but', 'other', 'businessmen', 'said', 'such', 'a', 'short', 'term', 'commercial', 'advantage', 'would', 'be', 'outweighed', 'by', 'further', 'u', 's', 'pressure', 'to', 'block', 'imports', 'that', 'is', 'a', 'very', 'short', 'term', 'view', 'said', 'lawrence', 'mills', 'director', 'general', 'of', 'the', 'federation', 'of', 'hong', 'kong', 'industry', 'if', 'the', 'whole', 'purpose', 'is', 'to', 'prevent', 'imports', 'one', 'day', 'it', 'will', 'be', 'extended', 'to', 'other', 'sources', 'much', 'more', 'serious', 'for', 'hong', 'kong', 'is', 'the', 'disadvantage', 'of', 'action', 'restraining', 'trade', 'he', 'said', 'the', 'u', 's', 'last', 'year', 'was', 'hong', 'kong', 's', 'biggest', 'export', 'market', 'accounting', 'for', 'over', 'pct', 'of', 'domestically', 'produced', 'exports', 'the', 'australian', 'government', 'is', 'awaiting', 'the', 'outcome', 'of', 'trade', 'talks', 'between', 'the', 'u', 's', 'and', 'japan', 'with', 'interest', 'and', 'concern', 'industry', 'minister', 'john', 'button', 'said', 'in', 'canberra', 'last', 'friday', 'this', 'kind', 'of', 'deterioration', 'in', 'trade', 'relations', 'between', 'two', 'countries', 'which', 'are', 'major', 'trading', 'partners', 'of', 'ours', 'is', 'a', 'very', 'serious', 'matter', 'button', 'said', 'he', 'said', 'australia', 's', 'concerns', 'centred', 'on', 'coal', 'and', 'beef', 'australia', 's', 'two', 'largest', 'exports', 'to', 'japan', 'and', 'also', 'significant', 'u', 's', 'exports', 'to', 'that', 'country', 'meanwhile', 'u', 's', 'japanese', 'diplomatic', 'manoeuvres', 'to', 'solve', 'the', 'trade', 'stand', 'off', 'continue', 'japan', 's', 'ruling', 'liberal', 'democratic', 'party', 'yesterday', 'outlined', 'a', 'package', 'of', 'economic', 'measures', 'to', 'boost', 'the', 'japanese', 'economy', 'the', 'measures', 'proposed', 'include', 'a', 'large', 'supplementary', 'budget', 'and', 'record', 'public', 'works', 'spending', 'in', 'the', 'first', 'half', 'of', 'the', 'financial', 'year', 'they', 'also', 'call', 'for', 'stepped', 'up', 'spending', 'as', 'an', 'emergency', 'measure', 'to', 'stimulate', 'the', 'economy', 'despite', 'prime', 'minister', 'yasuhiro', 'nakasone', 's', 'avowed', 'fiscal', 'reform', 'program', 'deputy', 'u', 's', 'trade', 'representative', 'michael', 'smith', 'and', 'makoto', 'kuroda', 'japan', 's', 'deputy', 'minister', 'of', 'international', 'trade', 'and', 'industry', 'miti', 'are', 'due', 'to', 'meet', 'in', 'washington', 'this', 'week', 'in', 'an', 'effort', 'to', 'end', 'the', 'dispute']\n",
      "['china', 'daily', 'says', 'vermin', 'eat', 'pct', 'grain', 'stocks', 'a', 'survey', 'of', 'provinces', 'and', 'seven', 'cities', 'showed', 'vermin', 'consume', 'between', 'seven', 'and', 'pct', 'of', 'china', 's', 'grain', 'stocks', 'the', 'china', 'daily', 'said', 'it', 'also', 'said', 'that', 'each', 'year', 'mln', 'tonnes', 'or', 'pct', 'of', 'china', 's', 'fruit', 'output', 'are', 'left', 'to', 'rot', 'and', 'mln', 'tonnes', 'or', 'up', 'to', 'pct', 'of', 'its', 'vegetables', 'the', 'paper', 'blamed', 'the', 'waste', 'on', 'inadequate', 'storage', 'and', 'bad', 'preservation', 'methods', 'it', 'said', 'the', 'government', 'had', 'launched', 'a', 'national', 'programme', 'to', 'reduce', 'waste', 'calling', 'for', 'improved', 'technology', 'in', 'storage', 'and', 'preservation', 'and', 'greater', 'production', 'of', 'additives', 'the', 'paper', 'gave', 'no', 'further', 'details']\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess the data\n",
    "sentences = []\n",
    "for fileid in reuters.fileids():\n",
    "    words = reuters.words(fileid)\n",
    "    sentences.append([word.lower() for word in words if word.isalpha()])\n",
    "\n",
    "# Display a few example sentences\n",
    "print(\"Example sentences:\")\n",
    "for i in range(2):\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a312ba68",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define and train the Word2Vec model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m Word2Vec(sentences, vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m70\u001b[39m, \n\u001b[1;32m      3\u001b[0m                  window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[1;32m      4\u001b[0m                  min_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[1;32m      5\u001b[0m                  sg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \n\u001b[1;32m      6\u001b[0m                  epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Display vocabulary size\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mwv))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gensim/models/word2vec.py:430\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, passes\u001b[38;5;241m=\u001b[39m(epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_vocab(corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, trim_rule\u001b[38;5;241m=\u001b[39mtrim_rule)\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(\n\u001b[1;32m    431\u001b[0m         corpus_iterable\u001b[38;5;241m=\u001b[39mcorpus_iterable, corpus_file\u001b[38;5;241m=\u001b[39mcorpus_file, total_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_count,\n\u001b[1;32m    432\u001b[0m         total_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcorpus_total_words, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs, start_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha,\n\u001b[1;32m    433\u001b[0m         end_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_alpha, compute_loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trim_rule \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gensim/models/word2vec.py:1073\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[0;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_begin(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1073\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch(\n\u001b[1;32m   1074\u001b[0m         corpus_iterable, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples,\n\u001b[1;32m   1075\u001b[0m         total_words\u001b[38;5;241m=\u001b[39mtotal_words, queue_factor\u001b[38;5;241m=\u001b[39mqueue_factor, report_delay\u001b[38;5;241m=\u001b[39mreport_delay,\n\u001b[1;32m   1076\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1077\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1078\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch_corpusfile(\n\u001b[1;32m   1079\u001b[0m         corpus_file, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, total_words\u001b[38;5;241m=\u001b[39mtotal_words,\n\u001b[1;32m   1080\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gensim/models/word2vec.py:1434\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch\u001b[0;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[1;32m   1431\u001b[0m     thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# make interrupting the process with ctrl+c easier\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 1434\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_epoch_progress(\n\u001b[1;32m   1435\u001b[0m     progress_queue, job_queue, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples,\n\u001b[1;32m   1436\u001b[0m     total_words\u001b[38;5;241m=\u001b[39mtotal_words, report_delay\u001b[38;5;241m=\u001b[39mreport_delay, is_corpus_file_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1437\u001b[0m )\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/gensim/models/word2vec.py:1289\u001b[0m, in \u001b[0;36mWord2Vec._log_epoch_progress\u001b[0;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[1;32m   1286\u001b[0m unfinished_worker_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unfinished_worker_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1289\u001b[0m     report \u001b[38;5;241m=\u001b[39m progress_queue\u001b[38;5;241m.\u001b[39mget()  \u001b[38;5;66;03m# blocks if workers too slow\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# a thread reporting that it finished\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m         unfinished_worker_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_empty\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/threading.py:355\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 355\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    356\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define and train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=40, \n",
    "                 window=5, \n",
    "                 min_count=5, \n",
    "                 sg=1, \n",
    "                 epochs=100)\n",
    "\n",
    "# Display vocabulary size\n",
    "print(\"Vocabulary size:\", len(model.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7e1a3ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Finding words similar to 'market'\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m similar_words \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwv\u001b[38;5;241m.\u001b[39mmost_similar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhockey\u001b[39m\u001b[38;5;124m'\u001b[39m, topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWords similar to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmarket\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word, score \u001b[38;5;129;01min\u001b[39;00m similar_words:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Finding words similar to 'market'\n",
    "similar_words = model.wv.most_similar('market', topn=10)\n",
    "print(\"Words similar to 'market':\")\n",
    "\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0c58201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Choose a subset of words for visualization\n",
    "words = ['market', 'economy', 'trade', 'finance', 'investment', 'growth', 'bank', 'money', 'stocks', 'currency']\n",
    "word_vectors = np.array([model.wv[word] \n",
    "                         for w in words \n",
    "                         if w in model.wv])\n",
    "\n",
    "# Reduce dimensions with t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=0, perplexity=2,)\n",
    "word_vectors_2d = tsne.fit_transform(word_vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577a869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the words and their vectors\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(word_vectors_2d[:, 0], word_vectors_2d[:, 1])\n",
    "\n",
    "# Annotate each point with the word\n",
    "for i, word in enumerate(words):\n",
    "    if word in model.wv:\n",
    "        plt.annotate(word, xy=(word_vectors_2d[i, 0], word_vectors_2d[i, 1]))\n",
    "\n",
    "plt.title(\"Word Embedding Visualization with t-SNE\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c76ae6",
   "metadata": {},
   "source": [
    "## Exploring word2vec\n",
    "* First, use two of your favorite word and identify the top 40 similar words.\n",
    "* Then transform those words into vectors.\n",
    "* Using Principal component analysis, plot the 80 words using MatPlotLib, coloring the two word groups differently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9078a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31002821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c30900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57abeff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
